{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9281b731",
   "metadata": {},
   "source": [
    "# 1. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d94e60ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ea7323d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/clark/Desktop/ironhack/shark attack/attacks.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Loading the Shark Attack dataset\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/clark/Desktop/ironhack/shark attack/attacks.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m\"\u001b[39m, sep\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1662\u001b[0m     f,\n\u001b[0;32m   1663\u001b[0m     mode,\n\u001b[0;32m   1664\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1665\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1666\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1667\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1668\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1669\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1670\u001b[0m )\n\u001b[0;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    860\u001b[0m             handle,\n\u001b[0;32m    861\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    862\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    863\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    864\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    865\u001b[0m         )\n\u001b[0;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/clark/Desktop/ironhack/shark attack/attacks.csv'"
     ]
    }
   ],
   "source": [
    "#Loading the Shark Attack dataset\n",
    "df = pd.read_csv(\"/Users/clark/Desktop/ironhack/shark attack/attacks.csv\", encoding=\"latin1\", sep= \",\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40d94e4",
   "metadata": {},
   "source": [
    "#### -> Renaming columns and dropping unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf1ea3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking what columns we have\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782808a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking how many rows we have \n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fdad43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standadarzing column names\n",
    "df.columns = df.columns.str.lower().str.replace(' ', '_').str.strip('_')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33231c0a",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------\n",
    "The following columns don't give us many insights for our prediction \n",
    "so we'll remove them from our data frame: \n",
    "\n",
    "'Investigator or Source', 'pdf', 'href formula', 'href',\n",
    "\n",
    "'Case Number.1', 'Case Number.2', 'original order', 'Unnamed: 22','Unnamed: 23'\n",
    "\n",
    "-------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e3c2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping unnecessary columns\n",
    "df.drop(['investigator_or_source', 'pdf', 'href_formula', 'href',\n",
    "       'case_number.1', 'case_number.2', 'original_order', 'unnamed:_22',\n",
    "       'unnamed:_23'], axis=1, inplace=True)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351da8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b984afb4",
   "metadata": {},
   "source": [
    "#### -> Checking for NaNs and standardizing each column if necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2db8ff",
   "metadata": {},
   "source": [
    "##### 1. Case Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ed4272",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking if we can drop all the rows in which case_number is null\n",
    "df[df['case_number'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb3142e",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------\n",
    "Since the entire rows are made of NaNs when case_number is null we'll be dropping these rows as\n",
    "they don't add any insights to our model\n",
    "----------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa39f29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the rows that all of them are Nans\n",
    "df.dropna(how='all', inplace=True)\n",
    "df['case_number'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91ebcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We still have one null\n",
    "df[df['case_number'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a77a1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking if when case_number is 0, all the other columns are also NaNs\n",
    "df[df['case_number'].str.strip() == '0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0a23f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping all rows in which the columns are all null but in case_number\n",
    "df.dropna(subset=df.columns.difference(['case_number']), how='all', inplace=True)\n",
    "df[df['case_number'].str.strip() == '0']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ae323d",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------\n",
    "case_number is still having however weird values, such as dates and letters. \n",
    "\n",
    "To standardize \n",
    "this column we can replace it by the number of the row plus one (row_number + 1), so that all rows are unique\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2b0150",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing case_number with the row index + 1 (so the first case_number isn't 0 or NaN)\n",
    "df['case_number'] = df.index + 1\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cef488",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking if case_number is now an int instead of object\n",
    "df['case_number'].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf221b75",
   "metadata": {},
   "source": [
    "##### 2. Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80825c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funtion that turns all values in date into actual dates\n",
    "from dateutil import parser\n",
    "\n",
    "def convert_to_datetime(df, column_name):\n",
    "    def parse_date(row):\n",
    "        try:\n",
    "            # Try to parse the date using dateutil.parser\n",
    "            parsed_date = parser.parse(row[column_name], dayfirst=True)\n",
    "            \n",
    "            # Check if the year is before 1677 \n",
    "            #(The default range for pandas datetime objects is from 1677-09-21 00:12:43.145225 \n",
    "            #to 2262-04-11 23:47:16.854775.),otherwise it returns out of bounds error\n",
    "            if parsed_date.year < 1677:\n",
    "                # If before 1677, return '01-01-1677'\n",
    "                return '01-01-1678'\n",
    "            \n",
    "            # Extract only the date part from the parsed datetime object\n",
    "            date_only = parsed_date.strftime('%d-%m-%Y')\n",
    "            \n",
    "            return date_only\n",
    "        \n",
    "        except Exception:\n",
    "            # If there is an error in parsing, return a default value\n",
    "            return '01-01-1900'  # Adjust the default value as per your requirement\n",
    "    \n",
    "    df[column_name] = df.apply(parse_date, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680053ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting all dates into actual dates using the previous function\n",
    "df = convert_to_datetime(df, 'date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3a273c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking row 4644 because it returns an error in the next cell due to this date\n",
    "print(df.iloc[4644])\n",
    "#Replacing the date for this specif row it its correct year\n",
    "df.loc[4644, 'date'] = '22-07-1944'\n",
    "print('----------------------')\n",
    "print(df.iloc[4644])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4001925b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting column date to datetime\n",
    "df['date'] = pd.to_datetime(df['date'], format='%d-%m-%Y')\n",
    "print(df['date'].dtype)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f49604",
   "metadata": {},
   "source": [
    "##### 3. Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7affef77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace non-finite values with a specific value (e.g., 1900) and converting column year from float to int\n",
    "df['year'] = df['year'].replace([np.nan, 0], 1900)\n",
    "df['year'] = df['year'].astype(int)\n",
    "df['year'].dtype\n",
    "\n",
    "df['year'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e1b159",
   "metadata": {},
   "source": [
    "##### 4. Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f35098",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that replaces the type with 'Sea Disaster' for types with the word 'Boat'\n",
    "def replace_type(df, column_name):\n",
    "    # Replace NaN values with 'Invalid'\n",
    "    df[column_name] = df[column_name].fillna('Invalid')\n",
    "    \n",
    "    # Use string methods to check if 'boat' is present in 'type' column (ignore case)\n",
    "    condition = df[column_name].str.contains('boat', case=False)\n",
    "    \n",
    "    # Replace values that meet the condition with 'Sea Disaster'\n",
    "    df.loc[condition, column_name] = 'Questionable'\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d40f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standadizing the types to have only: 'Unprovoked', 'Provoked', 'Invalid', 'Sea Disaster', 'Questionable'\n",
    "df = replace_type(df, 'type')\n",
    "df['type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7003fcdc",
   "metadata": {},
   "source": [
    "##### 5. Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaff744",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making sure all countries are set with upper case\n",
    "df['country'] = df['country'].str.upper().str.replace(r'\\W', ' ', regex=True).str.strip(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b50c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the display options to show all values without truncation\n",
    "pd.set_option('display.max_rows', None)\n",
    "df['country'].value_counts(dropna=False).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5909acb1",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------\n",
    "Because we have some invalid countries (eg. 'BETWEEN PORTUGAL & INDIA', 'DIEGO GARCIA', ) and a few NaN\n",
    "we'll check if these countris are on the all_countries pandas library and if not mark as 'UNKNOWN'\n",
    "\n",
    "---------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e629e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Installing and importing geonamescache library to have a list of all countries \n",
    "#to compare with our column country\n",
    "!pip install geonamescache\n",
    "import geonamescache\n",
    "\n",
    "gc = geonamescache.GeonamesCache()\n",
    "all_countries = list(gc.get_countries_by_names().keys())\n",
    "\n",
    "# Add 'Djibouti' to the list\n",
    "all_countries.append('Djibouti')\n",
    "\n",
    "#Turning all countries upper case so we can compare later with our country column\n",
    "all_countries = [country.upper() for country in all_countries]\n",
    "print(all_countries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0d7751",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------\n",
    "The bellow function iterates over each row of the DataFrame using iterrows(). \n",
    "\n",
    "-> It checks if the value in the \"country\" column exists in the all_countries list:\n",
    "\n",
    "    1. If it doesn't, it checks if the value in the \"area\" column contains a country that exists in the all_countries list or if country contains a country in the all_countries list. \n",
    "\n",
    "    2. If it does, it replaces the value in the \"country\" column with that extracted_country name using df.at[index, 'country'] = extracted_country.\n",
    "\n",
    "    3. If the extracted_country is not found in all_countries, it replaces the country with the string 'UNKNOWN' using df.at[index, 'country'] = 'UNKNOWN'.\n",
    "\n",
    "----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65ba62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing regex\n",
    "import re\n",
    "\n",
    "#Function to replaces the country name \n",
    "#If a value doesn't exist, it will check if the country name appears in the \"area\" column. \n",
    "#If it does, it will replace the value with the country name.\n",
    "def replace_country(df, country_column, area_column, all_countries):\n",
    "    # Replace NaN values with 'Unknown'\n",
    "    df[country_column] = df[country_column].fillna('UNKNOWN')\n",
    "    \n",
    "    #Iterating over each row of the DataFrame using iterrows()\n",
    "    for index, row in df.iterrows():\n",
    "        country = str(row[country_column])  # Ensure the value is a string\n",
    "        area = str(row[area_column])  # Ensure the value is a string\n",
    "        area = area.upper() #Ensuring it is in upper case like all_countries\n",
    "        \n",
    "        #Check if Country is \"OKINAWA\" or \"CEYLON\" replace with \"JAPAN\" and \"SRI LANKA\", respectively\n",
    "        if country == 'OKINAWA':\n",
    "            country = 'JAPAN'\n",
    "        elif country == 'CEYLON':\n",
    "            country = 'SRI LANKA'\n",
    "        elif country == 'USA':\n",
    "            country = 'UNITED STATES'\n",
    "        \n",
    "        # Check if any country name from all_countries exists in the area/country and extracts the country\n",
    "        pattern = r\"\\b(\" + \"|\".join(all_countries) + r\")\\b\"\n",
    "        match_country = re.search(pattern, country, flags=re.IGNORECASE)\n",
    "        match_area = re.search(pattern, area, flags=re.IGNORECASE)\n",
    "        \n",
    "        if match_country:\n",
    "            extracted_country = match_country.group(0)\n",
    "        else:\n",
    "            if match_area:\n",
    "                extracted_country = match_area.group(0)\n",
    "            else:\n",
    "                extracted_country = None\n",
    "        \n",
    "        #Checking if the value in the \"country\" column exists in the all_countries list\n",
    "        if country in all_countries:\n",
    "            df.at[index, country_column] = country\n",
    "        #If it doesn't, it checks if the value in the \"area\" column exists in the all_countries list\n",
    "        else:\n",
    "            if extracted_country in all_countries: \n",
    "                df.at[index, country_column] = extracted_country\n",
    "            #Otherwise it replaces the country with the string 'UNKNOWN'\n",
    "            else:\n",
    "                df.at[index, country_column] = 'UNKNOWN'\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b268ab2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing the countries with valid values using the previous function\n",
    "df = replace_country(df,'country', 'area', all_countries)\n",
    "pd.set_option('display.max_rows', None)\n",
    "df['country'].value_counts(dropna=False).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863f79c4",
   "metadata": {},
   "source": [
    "##### 6. Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b61cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making sure all areas are set with upper case\n",
    "df['area'] = df['area'].str.upper().str.replace(r'\\W', ' ', regex=True).str.strip(' ')\n",
    "\n",
    "# Set the display options to show all values without truncation\n",
    "df['area'].value_counts(dropna=False).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9041df0e",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------\n",
    "Since we're having a lot of distinct values, we'll check:\n",
    "\n",
    "    1. if the area cointains an existing region in the pycountry library and replace area with {region_name}, {country}. \n",
    "\n",
    "    2. if not we'll check if it has the word \"North\", \"Central\", \"South\", \"East\", \"West\" and change area for {North, Central, South, East, West},{COUNTRY}\n",
    "--------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82eeb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Installing it\n",
    "!pip install pycountry\n",
    "\n",
    "#Importing a Python package that provides a comprehensive collection of country-related data. \n",
    "import pycountry\n",
    "\n",
    "#Function that returns a list of regions for a given country\n",
    "def get_cities_by_country(country_name):\n",
    "    country_code = None\n",
    "    subdivisions_list = []  # Initialize an empty list to store the subdivision names\n",
    "    \n",
    "    for country in pycountry.countries:\n",
    "        if country.name == country_name:\n",
    "            country_code = country.alpha_2\n",
    "            break\n",
    "    \n",
    "    if country_code:\n",
    "        for subdivision in pycountry.subdivisions.get(country_code=country_code):\n",
    "            subdivisions_list.append(subdivision.name)  # Add subdivision name to the list\n",
    "        \n",
    "        return subdivisions_list  # Return the list of subdivision names\n",
    "    else:\n",
    "        return ''  # Return empty list if country not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92cdcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that replaces the area with a valid region, \n",
    "#or area (North, Central, South, East, West) and country_name\n",
    "def replace_area(df, country_column, area_column):\n",
    "    #List of regions\n",
    "    regions = []\n",
    "    \n",
    "    #Iterating over each row of the DataFrame using iterrows()\n",
    "    for index, row in df.iterrows():\n",
    "        # Ensure the value is a string and turns 'SRI LANKA' into 'Sri Lanka', for example\n",
    "        country = str(row[country_column]).title()  \n",
    "        # Ensure the value is a string\n",
    "        area = str(row[area_column]).title()  \n",
    "        \n",
    "        regions = [city for city in get_cities_by_country(country)]\n",
    "        \n",
    "        extracted_region = None\n",
    "        \n",
    "        #In case there's a list of regions we extract the area\n",
    "        if regions != []:\n",
    "            # Check if area cointains a region in the list\n",
    "            pattern = r\"\\b(\" + \"|\".join(map(re.escape, regions)) + r\")\\b\"\n",
    "            match_area = re.search(pattern, area, flags=re.IGNORECASE)\n",
    "\n",
    "            if match_area:\n",
    "                extracted_region = match_area.group(0) + ', ' + country\n",
    "            #If there are no matches in regions list, then we check if words in area are within a region\n",
    "            #For example 'Veracruz' should match 'Veracruz de Ignacio de la Llave'\n",
    "            else:\n",
    "                pattern = r\"\\b\" + re.escape(area) + r\"\\b\"\n",
    "\n",
    "                for region in regions:\n",
    "                    if re.search(pattern, region, flags=re.IGNORECASE):\n",
    "                        extracted_region = region + ', ' + country\n",
    "                        break\n",
    "            \n",
    "            #In case there were no matches at all we check if there's key words like \n",
    "            #'north', 'south', 'west', 'east', 'central'\n",
    "            if extracted_region == None:\n",
    "                pattern = r\"(north|south|west|east|central)\"\n",
    "                match_area = re.search(pattern, area, flags=re.IGNORECASE)\n",
    "\n",
    "                if match_area:\n",
    "                    extracted_region = match_area.group(0) + ', ' + country\n",
    "    \n",
    "        #Replacing the area with the extracted_region in case there was a match\n",
    "        if extracted_region != None:\n",
    "            df.at[index, area_column] = extracted_region\n",
    "        else:\n",
    "            df.at[index, area_column] = 'UNKNOWN'\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ab466e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing the areas with valid values using the previous function\n",
    "df = replace_area(df,'country', 'area')\n",
    "pd.set_option('display.max_rows', None)\n",
    "df['area'].value_counts(dropna=False).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ac71f9",
   "metadata": {},
   "source": [
    "##### 7. Location"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17aac2b",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------\n",
    "Location might not be a relevant column for us, so we'll drop eventually possibly\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362b6b91",
   "metadata": {},
   "source": [
    "##### 8. Activity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edd25d7",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------\n",
    "For column 'activity' we'll divide it into categories: \n",
    "\n",
    "-> swimming: including any injury containing words 'swimming', 'swimm', 'bathing', 'dangling', 'floating'\n",
    "\n",
    "-> diving: including words such as 'dive', 'diving', 'scuba', 'dived''\n",
    "\n",
    "-> fishing: 'chase', 'catch','catching', 'attract', 'attracting','fishing', 'fish', 'retrieve', 'net', 'collecting', 'crabbing', 'crayfishing', 'feeding', 'rescuing', 'rescue','trap\n",
    "\n",
    "-> water sport: 'board', 'canoe', 'surf', 'surfing', 'boarding', 'canoeing', 'paddle', 'paddling'\n",
    "\n",
    "-> sailing: 'boat', 'sailing', 'sail', 'ship', 'sailboat', 'adrift', 'adrifting', 'conducting', 'cruise', 'cruising', 'anchor', 'escape', 'escaping', 'yacht','yachting'\n",
    "\n",
    "-> air disaster: 'air', 'aircraft', 'crashed'\n",
    "\n",
    "-> unknown: For anything else\n",
    "\n",
    "---------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc83df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that replaces the activity with its respective category\n",
    "def replace_activity(df, activity_column):\n",
    "    for index, row in df.iterrows():\n",
    "        activity = str(row[activity_column]).lower()  # Convert to lowercase for case insensitivity\n",
    "        \n",
    "        \n",
    "        # Check for specific keywords/categories using string matching\n",
    "        if any(keyword in activity for keyword in ['swimming', 'swimm', 'bathing', 'dangling', 'floating']):\n",
    "            df.at[index, activity_column] = 'swimming'\n",
    "\n",
    "        elif any(keyword in activity for keyword in ['dive', 'diving', 'scuba', 'dived']):\n",
    "            df.at[index, activity_column] = 'diving' \n",
    "\n",
    "        elif any(keyword in activity for keyword in ['chase', 'catch','catching', 'attract', \n",
    "                                                     'attracting','fishing', 'fish', 'retrieve', 'net', \n",
    "                                                     'collecting', 'crabbing', 'crayfishing', 'feeding', \n",
    "                                                     'rescuing', 'rescue','trap']):\n",
    "            df.at[index, activity_column] = 'fishing'\n",
    "        \n",
    "        elif any(keyword in activity for keyword in ['board', 'canoe', 'surf', 'surfing', 'boarding', \n",
    "                                                     'canoeing', 'paddle', 'paddling']):\n",
    "            df.at[index, activity_column] = 'water sport'\n",
    "        \n",
    "        elif any(keyword in activity for keyword in ['boat', 'sailing', 'sail', 'ship', 'sailboat', \n",
    "                                                     'adrift', 'adrifting', 'conducting', 'cruise', \n",
    "                                                     'cruising', 'anchor', 'escape', 'escaping', 'yacht',\n",
    "                                                     'yachting']):\n",
    "            df.at[index, activity_column] = 'sailing'\n",
    "            \n",
    "        elif any(keyword in activity for keyword in ['air', 'aircraft', 'crashed']):\n",
    "            df.at[index, activity_column] = 'air disaster'\n",
    "\n",
    "        else:\n",
    "            df.at[index, activity_column] = 'unknown'\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3af1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing the activities using the above function\n",
    "df = replace_activity(df, 'activity')\n",
    "df['activity'].value_counts(dropna=False).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5547fd1",
   "metadata": {},
   "source": [
    "##### 9. Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08247d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = df['name'].value_counts()\n",
    "\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff96228",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------\n",
    "Like Location name might not be very relevant for our model, therefore we'll probably drop it too \n",
    "\n",
    "------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddada135",
   "metadata": {},
   "source": [
    "##### 10. Sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d7b4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing 'sex' column with 'm', 'f' and 'o'\n",
    "df['sex'] = df['sex'].replace({'M': 'm', 'F': 'f', 'N': 'o', 'lli': 'o', '.': 'o', 'M ': 'm'})\n",
    "df['sex'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e88a3ea",
   "metadata": {},
   "source": [
    "##### 11. Age"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c65fbd2",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------\n",
    "For age we'll divide all ges into categories such as:\n",
    "\n",
    "-> child: 0-12\n",
    "\n",
    "-> teenage: 13-17\n",
    "\n",
    "-> young Adult: 18-30\n",
    "\n",
    "-> adult: 30-49\n",
    "\n",
    "-> middle-age: 50-64\n",
    "\n",
    "-> elderly: 65+\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7800c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that replaces the ages whith its age category\n",
    "def replace_age(age):\n",
    "    if pd.isnull(age):  # Check for missing values\n",
    "        return 'unknown'\n",
    "    age = age.lower()  # Convert to lowercase for case insensitivity\n",
    "    \n",
    "    # Check for specific patterns using regular expressions\n",
    "    if re.search(r'\\d+', age):  # Check for any digit in the string\n",
    "        age_number = int(re.findall(r'\\d+', age)[0])  # Extract the first digit as the age number\n",
    "        \n",
    "        if re.search(r'mid-\\d+s', age):  # Check for the 'mid-30s' or 'mid-20s' pattern\n",
    "            age_number = age_number + 5  # Extract the number from the pattern and 5 more years\n",
    "            if age_number >= 20 and age_number <= 30:\n",
    "                return 'young-adult'\n",
    "            elif age_number >= 30 and age_number <= 49:\n",
    "                return 'adult'\n",
    "            else:\n",
    "                return 'unknown'\n",
    "        \n",
    "        elif age_number >= 0 and age_number <= 12:\n",
    "            return 'child'\n",
    "        elif age_number >= 13 and age_number <= 17:\n",
    "            return 'teenager'\n",
    "        elif age_number >= 18 and age_number <= 30:\n",
    "            return 'young-adult'\n",
    "        elif age_number >30 and age_number <= 49:\n",
    "            return 'adult'\n",
    "        elif age_number >= 50 and age_number <= 64:\n",
    "            return 'middle-age'\n",
    "        elif age_number >= 65:\n",
    "            return 'elderly'\n",
    "        else:\n",
    "            return 'unknown'\n",
    "    \n",
    "    elif re.search(r'child', age):  # Check for the presence of 'child' in the string\n",
    "        return 'child'\n",
    "    \n",
    "    elif re.search(r'teen', age):  # Check for the presence of 'teen' in the string\n",
    "        return 'teenager'\n",
    "    \n",
    "    elif age.startswith('young'):\n",
    "        return 'young-adult'\n",
    "    \n",
    "    elif re.search(r'adult', age):  # Check for the (adult)' pattern\n",
    "        return 'adult'\n",
    "    \n",
    "    elif age.startswith('middle'):\n",
    "        return 'middle-age'\n",
    "    \n",
    "    else:\n",
    "        return 'unknown'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc529f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying replace_age funtion to the 'age' column\n",
    "df['age'] = df['age'].apply(replace_age)\n",
    "\n",
    "# Print the updated DataFrame\n",
    "df['age'].value_counts(dropna=False).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04527cae",
   "metadata": {},
   "source": [
    "##### 12. Injury"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb16eb08",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------\n",
    "For column injury we'll also divide it into categories: \n",
    "\n",
    "-> injured: including any injury containing words 'injured', 'laceration', 'bite', 'wound', 'gash',\n",
    "    'scratch', 'cut','bitten', 'mauled', 'teeth', 'recovered','serious', 'tooth', 'puncture', 'severed'\n",
    "\n",
    "-> dead: including words such as 'perish', 'dead', 'death', 'body', 'bodies', 'lost', 'remains'\n",
    "\n",
    "-> no injury: 'hoax', 'no injury', 'survived', 'survive'\n",
    "\n",
    "-> unknown: For anything else\n",
    "\n",
    "----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22359efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fuction that replaces the injury with a set category given a keyword \n",
    "def replace_injury(df, injury_column):\n",
    "    for index, row in df.iterrows():\n",
    "        injury = str(row[injury_column]).lower()  # Convert to lowercase for case insensitivity\n",
    "\n",
    "        # Check for specific keywords/categories using string matching\n",
    "        if any(keyword in injury for keyword in ['injured', 'laceration', 'lacerated','bite', 'wound', 'gash', 'scratch', \n",
    "                                                   'injuries', 'injury','cut', 'bitten', 'mauled', 'teeth', 'recovered', 'serious', \n",
    "                                                   'tooth', 'puncture', 'severed']):\n",
    "            if any(keyword in injury for keyword in ['no injury']):\n",
    "                df.at[index, injury_column] = 'no injury'\n",
    "            elif any(keyword in injury for keyword in ['perish', 'perished', 'dead', 'death', 'body', 'bodies', 'lost', 'remains', 'died']):\n",
    "                df.at[index, injury_column] = 'dead'\n",
    "            else:\n",
    "                df.at[index, injury_column] = 'injured'\n",
    "\n",
    "        elif any(keyword in injury for keyword in ['hoax', 'no injury','survived', 'survive']):\n",
    "            if any(keyword in injury for keyword in ['perish', 'perished','dead', 'death', 'body', 'bodies', 'lost', 'remains', 'died']):\n",
    "                df.at[index, injury_column] = 'dead'\n",
    "            else:\n",
    "                df.at[index, injury_column] = 'no injury'\n",
    "\n",
    "        elif any(keyword in injury for keyword in ['perish', 'dead', 'death', 'body', 'bodies', 'lost', 'remains', 'died']):\n",
    "            df.at[index, injury_column] = 'dead'\n",
    "\n",
    "        else:\n",
    "            df.at[index, injury_column] = 'unknown'\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dad9acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying the previous function to the 'injury' column\n",
    "df = replace_injury(df, 'injury')\n",
    "df['injury'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adea6733",
   "metadata": {},
   "source": [
    "##### 13. Fatal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a05426",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Renaming column fatalas we are adding 'u' for unknown\n",
    "df = df.rename(columns={\"fatal_(y/n)\": \"fatal(y/n/u)\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5833b2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['fatal(y/n/u)'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbfdfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing values with a valid value\n",
    "df['fatal(y/n/u)'] = df['fatal(y/n/u)'].replace({'UNKNOWN': 'u', 'M': 'u', ' N': 'n', '2017': 'u', 'N ': 'n', 'Y': 'y', 'N': 'n'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aafe3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['fatal(y/n/u)'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08cac51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filling the NaNs with 'u'\n",
    "df.fillna('u', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c867a9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['fatal(y/n/u)'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ae2e1c",
   "metadata": {},
   "source": [
    "##### 14. Time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5985b76",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------\n",
    "For column time, we'll split into the following categories:\n",
    "\n",
    "Morning, Midday, Afternoon, Evening and Night\n",
    "\n",
    "------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff0c5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fuction that replaces the time with its category\n",
    "def standarize_time(value):\n",
    "    if pd.isna(value) or value == '--' or value == '':\n",
    "        return 'Unknown'\n",
    "    elif 'h' in value:\n",
    "        try:\n",
    "            hour = int(value.split('h')[0])\n",
    "            if 6 <= hour < 10:\n",
    "                return 'Morning'\n",
    "            elif 10 <= hour < 14:\n",
    "                return 'Midday'\n",
    "            elif 14 <= hour < 17:\n",
    "                return 'Afternoon'\n",
    "            elif 17 <= hour < 21:\n",
    "                return 'Evening'\n",
    "            else:\n",
    "                return 'Night'\n",
    "        except ValueError:\n",
    "            return 'Unknown'\n",
    "    elif 'Morning' in value or 'AM' in value:\n",
    "        return 'Morning'\n",
    "    elif 'Midday' in value:\n",
    "        return 'Midday'\n",
    "    elif 'Afternoon' in value or 'PM' in value:\n",
    "        return 'Afternoon'\n",
    "    elif 'Night' in value or 'Evening' in value:\n",
    "        return 'Night'\n",
    "    else:\n",
    "        return 'Unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d0f74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying the previous function to time\n",
    "df['time'] = df['time'].apply(standarize_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053c671a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['time'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a05df7",
   "metadata": {},
   "source": [
    "##### 15. Species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934456a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing the NaN with 'unknown'\n",
    "df['species'].fillna(\"unknown\", inplace=True)\n",
    "df.loc[~df['species'].str.contains('shark', case=False), 'species'] = 'unknown'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485df6fa",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------\n",
    "The Function bellow checks:\n",
    "\n",
    "    1. if the value contains the word 'shark'\n",
    "    \n",
    "        1.1 if it does then we fetch the word before shark and shark and save it as the species\n",
    "        1.2 if there's no other word before shark then we mark as 'unknown'\n",
    "        1.3 if the string value doesn't contain the word 'shark' then we mark as 'unknown'\n",
    "        \n",
    "--------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa09b6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fuction that fetches the species from the strings that compose column species\n",
    "def replace_species(df, species_column):\n",
    "    for index, row in df.iterrows():\n",
    "        species = str(row[species_column]).lower() # Convert to lowercase for case insensitivity\n",
    "        \n",
    "        #Clearing numbers from the string\n",
    "        species = re.sub(r'\\d+', '', species)\n",
    "    \n",
    "        if 'shark' in species:\n",
    "            # Retrieve the word before 'shark' and the word 'shark' itself\n",
    "            excluded_words = ['small', 'large', 'big', 'another', 'from', 'foot', \n",
    "                              'same', 'two', 'young', 'old', 'female', 'male']\n",
    "            pattern = r'(\\b(?!(?:{}|\\d+)\\b)\\w{{3,}}\\s+shark\\b)'.format('|'.join(excluded_words))\n",
    "            match = re.search(pattern, species, re.IGNORECASE)\n",
    "            if match:\n",
    "                df.at[index, species_column] = match.group(1)\n",
    "            else:\n",
    "                df.at[index, species_column] = 'unknown'\n",
    "        else:\n",
    "            # Replace species without the word 'shark' with 'unknown'\n",
    "            df.at[index, species_column] = 'unknown'\n",
    "        \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929bc790",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying the above function to replace the species column with valid values\n",
    "df = replace_species(df, 'species')\n",
    "\n",
    "df['species'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e69f1c",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------\n",
    "Before doing the logistic regression and data analysis we are going to do some filtering for the countries and species:\n",
    "\n",
    "    1. Countries: we are going to filter it by the top 10 countries that had the most shark attacks\n",
    "    2. Species: we are going to filter out the species that only have 1 count\n",
    "        \n",
    "--------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096ccc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c50b576",
   "metadata": {},
   "source": [
    "**Countries**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a83a67",
   "metadata": {},
   "source": [
    "Creating a filter for countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4df9d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['country'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf566c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The top 10 countries are the following: United States, Australia, South Africa, Papua New Guinea, New Zealand, Brazil, Bahamas, Mexico, Italy and Unknown\n",
    "\n",
    "top_10 = ['UNITED STATES', 'AUSTRALIA', 'SOUTH AFRICA', 'UNKNOWN', 'PAPUA NEW GUINEA', 'NEW ZEALAND', 'BRAZIL', 'BAHAMAS', 'MEXICO', 'ITALY']\n",
    "df1_top_10 = df1[df1['country'].isin(top_10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bbce9e",
   "metadata": {},
   "source": [
    "**Species**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52002f11",
   "metadata": {},
   "source": [
    "Creating a filter for species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e7d45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are going to filter out all the type of species that the count is under 5\n",
    "\n",
    "species_count = df1['species'].value_counts()\n",
    "species_attacks = species_count[species_count > 4].index\n",
    "df1_species_attacks = df1[df1['species'].isin(species_attacks)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e744f46d",
   "metadata": {},
   "source": [
    "**Combine both**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2203d38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a8d147",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1_top_10[df1_top_10['species'].isin(species_attacks)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88570484",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['country'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb05b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['species'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88aa8b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping columns location and name as they wont't be relevant for our model\n",
    "df_filtered = df2.drop(['case_number','location','name', 'date'], axis=1, inplace=True)\n",
    "\n",
    "df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ce782e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
